{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "10.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "97szrJb-UKPg",
        "outputId": "984be136-24c1-4eb2-96b7-bed001aa0590",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "import pandas as pd\n",
        "import math \n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# A function to return a column of the data at the specified index\n",
        "def col(array, i):\n",
        "    return [row[i] for row in array]\n",
        "\n",
        "# A function to calculate the mean of an array\n",
        "def mean(array): \n",
        "    m = []\n",
        "    for i in range(4):\n",
        "        m.append(sum(col(array,i))/len(col(array,i)))\n",
        "    return m\n",
        "\n",
        "# a function to implement LRT\n",
        "def rule(x_ts,x,y):\n",
        "    p1 = len([i for (i, val) in enumerate(y) if val == 1])\n",
        "    p2 = len([i for (i, val) in enumerate(y) if val == 2])\n",
        "    p1, p2 = p1/(len(y)), p2/(len(y))\n",
        "    x1 = np.array([x[i] for (i, val) in enumerate(y) if val == 1])\n",
        "    x2 = np.array([x[i] for (i, val) in enumerate(y) if val == 2])\n",
        "    m1 = mean(x1)\n",
        "    m2 = mean(x2)\n",
        "    cov1 = np.cov(x1.T)\n",
        "    cov2 = np.cov(x2.T)\n",
        "    coeff1 = 1/(((2*3.14)**2)*np.linalg.det(cov1)**0.5)\n",
        "    coeff2 = 1/(((2*3.14)**2)*np.linalg.det(cov2)**0.5)\n",
        "    l1 = coeff1*np.exp(-0.5*np.dot(np.dot((x_ts - m1),np.linalg.inv(cov1)),(x_ts - m1).T))\n",
        "    l2 = coeff2*np.exp(-0.5*np.dot(np.dot((x_ts - m2),np.linalg.inv(cov2)),(x_ts - m2).T))\n",
        "    if (l1/p2) > (l2/p1):\n",
        "        return 1\n",
        "    else:\n",
        "        return 2\n",
        "    \n",
        "def confmat(y_pred,y_ts):\n",
        "    a, b, c, d = 0, 0, 0, 0\n",
        "    for i in range(len(y_ts)):\n",
        "        if y_ts[i] == 1:\n",
        "            if y_pred[i] == 1:\n",
        "                a = a + 1\n",
        "            if y_pred[i] == 2:\n",
        "                b = b + 1\n",
        "        if y_ts[i] == 2:\n",
        "            if y_pred[i] == 1:\n",
        "                c = c + 1\n",
        "            if y_pred[i] == 2:\n",
        "                d = d + 1\n",
        "    return a, b, c, d\n",
        "\n",
        "#Data\n",
        "data = pd.read_excel('data3.xlsx',header=None)\n",
        "data = data.sample(frac=1)\n",
        "data = np.asarray(data)\n",
        "y = data[:,-1]\n",
        "x = data[:,:-1]\n",
        "\n",
        "#Split into testing and training sets\n",
        "train_size = int(0.6 * len(x))\n",
        "x_tr = x[:train_size]\n",
        "x_ts = x[train_size:]\n",
        "y_tr = y[:train_size]\n",
        "y_ts = y[train_size:]\n",
        "\n",
        "y_pred = []\n",
        "for i in range(len(x_ts)):\n",
        "    y_pred.append(rule(x_ts[i],x_tr,y_tr))\n",
        "\n",
        "a, b, c, d = confmat(y_pred,y_ts)\n",
        "acc = (a+d)/(a+b+c+d)\n",
        "sens = (a)/(a+b)\n",
        "spec = (d)/(d+c)\n",
        "\n",
        "print('Assuming class 1 as positive and class2 as negative,')\n",
        "print('tp: ',a,'fp: ',c,'tn: ',d,'fn: ',b)\n",
        "print('accuracy: ',acc,'sensitivity: ',sens,'specificity: ',spec)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Assuming class 1 as positive and class2 as negative,\n",
            "tp:  22 fp:  0 tn:  18 fn:  0\n",
            "accuracy:  1.0 sensitivity:  1.0 specificity:  1.0\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}